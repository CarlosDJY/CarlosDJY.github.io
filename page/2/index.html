<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"carlosdjy.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.17.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://carlosdjy.github.io/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="CarlosDJY">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://carlosdjy.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">CarlosDJY</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Yolo-V2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Yolo-V2/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-06 15:56:05" itemprop="dateModified" datetime="2023-04-06T15:56:05+08:00">2023-04-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>[[Yolo-V2.pdf]]</p>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><h3 id="优化角度"><a href="#优化角度" class="headerlink" title="优化角度"></a>优化角度</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>加速收敛，防止过拟合，防止梯度消失</p>
<h5 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h5><p>对于一个 Batch 中神经元输出的响应值标准化<br>记录响应值的均值和方差<br>标准化后的响应值 × γ + β<br>每个神经元训练一组 γ 和 β</p>
<h5 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h5><p>均值、方差使用训练阶段均值与方差的期望<br>γ 和 β 使用每个训练得到的最终值全局推断求出<br>![[Batch Normalization for Test.png]]</p>
<h4 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h4><p>先在 ImageNet（224 × 224）上 Pretrain<br>再在放大后的ImageNet（448 × 448）上进一步 Pretrain<br>最后迁移到目标检测的448 × 448数据集上训练</p>
<h4 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h4><h5 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h5><p>图片划分为 13 × 13 个 Grid Cell<br>每个 Grid Cell 预测 5 个 Anchor（先验框）<br>预测框输出其相对对应 Anchor 的偏移量</p>
<h5 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h5><p>输出维数：13 × 13 × 125<br>每个 Anchor 输出 (x, y, h, w, c) 位置和置信度 + 20 个 Classes 概率分布<br>每个 Grid Cell 输出 125 个参数</p>
<h5 id="Cluster-IOU"><a href="#Cluster-IOU" class="headerlink" title="Cluster IOU"></a>Cluster IOU</h5><p>长宽比聚类 -&gt; 安排长宽比数据</p>
<h5 id="Anchor-位置限制"><a href="#Anchor-位置限制" class="headerlink" title="Anchor 位置限制"></a>Anchor 位置限制</h5><h6 id="图例"><a href="#图例" class="headerlink" title="图例"></a>图例</h6><p>![[YOLO V2 Anchor Restriction.png]]</p>
<h6 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h6><p>b$_x$ 和 b$_y$ 限制 Anchor 的中心点在 Grid Cell 内部<br>b$_w$ 和 b$_h$ 基于 Anchor 基准值缩放</p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><h6 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h6><p>![[YOLO V2 Loss Function.png]]</p>
<h6 id="函数参数"><a href="#函数参数" class="headerlink" title="函数参数"></a>函数参数</h6><p>1$_{MaxIOU &lt; Thresh}$</p>
<ul>
<li>Thresh 在文中为 0.6</li>
<li>如果 IOU 小于阈值则为 0，反之为 1</li>
<li>将 Anchor 中心点与 Ground Truth 中心点重合后计算 IOU</li>
<li>只考虑形状，不考虑位置<br>1$_t&lt;12800$</li>
<li>是否是模型训练的早期<br>1$_k^{truth}$</li>
<li>当前 Anchor 是否负责检测物体</li>
<li>如果是则为1，反之为0</li>
<li>IOU &gt; Thresh 但非最大的预测框忽略其损失</li>
</ul>
<h4 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h4><p>pooling 前：26 × 26 × 512</p>
<ul>
<li>拆分：4 个 13 × 13 × 512</li>
<li>池化 + 卷积：13 × 13 × 1024</li>
<li>叠加为 13 × 13 × 3072</li>
</ul>
<h4 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h4><p>输入不同尺度的图像加以训练</p>
<h3 id="分类强化"><a href="#分类强化" class="headerlink" title="分类强化"></a>分类强化</h3><h4 id="联合训练"><a href="#联合训练" class="headerlink" title="联合训练"></a>联合训练</h4><p>COCO：带目标检测标注，类别少，数据少<br>IMAGENET：仅分类标注，类别多，数据多</p>
<h6 id="WordTree1k"><a href="#WordTree1k" class="headerlink" title="WordTree1k"></a>WordTree1k</h6><p>![[YOLO V2 WordTree Structure.png]]</p>
<h6 id="分层-SoftMax-结构"><a href="#分层-SoftMax-结构" class="headerlink" title="分层 SoftMax 结构"></a>分层 SoftMax 结构</h6><ul>
<li>WordTree1k</li>
<li>进行图像分类</li>
<li>进一步进行目标识别</li>
</ul>
<h2 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><h4 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h4><h5 id="目标检测横向比对"><a href="#目标检测横向比对" class="headerlink" title="目标检测横向比对"></a>目标检测横向比对</h5><p>We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories.<br>我们发明了YOLO9000，这是一种先进的实时物体检测系统，可检测9000多个物体类别。<br>First we propose various improvements to the YOLO detection method, both novel and drawn from prior work.<br>首先，我们对YOLO检测方法提出了各种改进，既新颖又借鉴了先前的工作。<br>The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO.<br>改进型YOLOv2在PASCAL VOC和COCO等标准检测任务上是最先进的。<br>Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy.<br>使用一种新颖的多尺度训练方法，相同的YOLOv2模型可以以不同的大小运行，从而在速度和精度之间提供了一种简单的折衷。<br>At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster.<br>YOLOv2以67FPS的速度在VOC 2007上获得76.8mAP。YOLOv2的帧速率为40FPS，达到78.6mAP，超过了最先进的方法，如Faster R-CNN、ResNet和SSD，同时运行速度明显更快。</p>
<h5 id="多类别目标检测"><a href="#多类别目标检测" class="headerlink" title="多类别目标检测"></a>多类别目标检测</h5><p>Finally we propose a method to jointly train on object detection and classification.<br>最后，我们提出了一种基于目标检测和分类的联合训练方法。<br>Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset.<br>使用该方法，我们在COCO检测数据集和ImageNet分类数据集上同时训练YOLO9000。<br>Our joint training allows YOLO9000 to predict detections for object classes that don’t have labelled detection data.<br>我们的联合训练使YOLO9000能够预测没有标记检测数据的对象类的检测。<br>We validate our approach on the ImageNet detection task.<br>我们在ImageNet检测任务中验证了我们的方法。<br>YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes.<br>YOLO9000在ImageNet检测验证集上获得19.7mAP，尽管200个类别中只有44个类别的检测数据。<br>On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP.<br>在不在COCO的156个类别中，YOLO9000获得16.0mAP。<br>But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.<br>但YOLO可以检测到200多个类；它预测了超过9000种不同对象类别的检测，且它仍然实时运行。</p>
<h4 id="摘要重点"><a href="#摘要重点" class="headerlink" title="摘要重点"></a>摘要重点</h4><p>基于之前的 YOLO V1 加以改进，使其在 PASCAL VOC 和 COCO 等常用标准检测任务上具有最高的准确率，同时进一步提升运行速度。<br>通过多尺度训练，让 YOLO V2 可以在速度和精度间灵活取舍。<br>通过联合训练，拓展 YOLO V2 的识别类别。</p>
<h3 id="论文十问"><a href="#论文十问" class="headerlink" title="论文十问"></a>论文十问</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>对 YOLO V1 进行进一步优化，增加其准确度、对于小目标的识别能力和检出全部目标的能力。<br>拓展 YOLO 目标检测的范围，不增加带标注的目标检测集，而使其能用于更多的类型。</p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>在当时其他常用目标检测网络虽然具有较好的准确率，但检测速度仍然较慢，与此同时 YOLO V1 的准确度相对较低，具有较大的改进空间。</p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>通过以下模型的构建，能够在有高检测速度的同时不失其准确率，并且进一步扩展其目标识别的物体范围。</p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><h5 id="Inception-V4-和-InceptionResNet-V2"><a href="#Inception-V4-和-InceptionResNet-V2" class="headerlink" title="Inception-V4 和 InceptionResNet-V2"></a>Inception-V4 和 InceptionResNet-V2</h5><p>[[Inception-V4 and InceptionResNet-V2]]<br>C. Szegedy, S. Ioffe, and V. Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, abs&#x2F;1602.07261, 2016.</p>
<h5 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h5><p>R. B. Girshick. Fast R-CNN. CoRR, abs&#x2F;1504.08083, 2015.</p>
<h5 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h5><p>S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015. 5.</p>
<h5 id="Network-in-Network"><a href="#Network-in-Network" class="headerlink" title="Network-in-Network"></a>Network-in-Network</h5><p>M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.</p>
<h5 id="YOLO-V1"><a href="#YOLO-V1" class="headerlink" title="YOLO-V1"></a>YOLO-V1</h5><p>[[Yolo-V1]]<br>J. Redmon, S. Divvala, R. Girshick, A. Farhadi. You Only Look Once: Unified, Real-Time Object Detection. arXiv:1506.02640, 2015.</p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>在准确度优化方面，本文使用了批归一化，高分辨率分类器，基准 Bounding Box，细粒度特征，用以提升综合识别准确率，并改善其对于小目标的识别准确率。<br>在拓展识别分类方面，本文结合 ImageNet 和 COCO，通过构建 WordTree1k，运用分层 Softmax 结构，以期获得更好的多分类识别能力。</p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>见上述[[#整体结构]]部分。</p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>数据集为 PASCAL VOC 2007 + 2012，COCO，ImageNet。<br>代码已开源：<a target="_blank" rel="noopener" href="https://pjreddie.com/darknet/yolo/">YOLO: Real-Time Object Detection</a></p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>在 YOLO-V1 的基础上显著提升了准确率。（63.4%到78.6%）<br>该模型在目标检测领域准确度达到了当时的最高水平。（最高74.9%，该模型73.4%）<br>该模型在计算速度上远超其他模型，包括其前代 YOLO-V1。（YOLO-V1的2倍，Faster R-CNN的12倍）<br>该模型能够通过改变输入图片的尺寸，在精度与速度之间进行取舍。<br>该模型通过半监督学习，在 ImageNet 上对动物等物体具有较好的目标检测准确度。</p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>对 YOLO-V1 作出了一定的改进，弥补了其准确率和小目标识别能力的弱点，同时进一步提升了其运行速度；与此同时，给出了一种基于半监督学习的拓展方法，使得目标检测不必依赖于大量标注数据也能获得较好的效果。</p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>进一步改进其识别准确度。<br>改进对于多分类的识别能力，尤其是物品方面。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Yolo-V1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Yolo-V1/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-06 15:55:55" itemprop="dateModified" datetime="2023-04-06T15:55:55+08:00">2023-04-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>[[Yolo-V1.pdf]]</p>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><h3 id="目标检测算法"><a href="#目标检测算法" class="headerlink" title="目标检测算法"></a>目标检测算法</h3><h4 id="预测阶段"><a href="#预测阶段" class="headerlink" title="预测阶段"></a>预测阶段</h4><h5 id="前向推断"><a href="#前向推断" class="headerlink" title="前向推断"></a>前向推断</h5><p>输出层：7 × 7 × 30</p>
<ul>
<li>30 &#x3D; 2 × 5 + 20<ul>
<li>2：2个 Bounding Box</li>
<li>5：5个 Bounding Box 参数</li>
<li>20：20个分类类别的概率<br>输入图片 -&gt; 7 × 7个 Grid Cell<br>每个 Grid Cell 预测2个 Bounding Box</li>
</ul>
</li>
<li>(x, y, h, w, c)</li>
<li>(x, y): 中心点坐标（在 Grid Cell 内）</li>
<li>(h, w): Bounding Box 的高度和宽度</li>
<li>(c): 置信度<br>每个 Grid Cell 预测一组条件类别概率<br>置信度 × 条件类别概率 &#x3D; Bounding Box 类别概率</li>
</ul>
<h5 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h5><h6 id="置信度过滤"><a href="#置信度过滤" class="headerlink" title="置信度过滤"></a>置信度过滤</h6><p>7 × 7 × 30 -&gt; 目标检测结果<br>置信度 × 条件类别概率 &#x3D; Bounding Box 类别概率<br>每个 Grid Cell 获得2 × 20的全概率<br>获得98个1 × 20的全概率<br>对于检测类别按概率降序排列</p>
<h6 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h6><p>对于概率值最大的 Bounding Box<br>计算和其余 Bounding Box 的 IoU值 (Intersection over Union)<br>如果 IoU &gt; 0.5 则认为是同一个物体<br>忽略 IoU 较小的 Bounding Box，并将其概率置0<br>再考虑概率值第二大的，依此类推<br>对所有20个类别进行非极大值抑制</p>
<h4 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h4><p>人工标注 Ground Truth</p>
<h5 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h5><h6 id="结构示意图"><a href="#结构示意图" class="headerlink" title="结构示意图"></a>结构示意图</h6><p>![[Yolo V1 structure.png]]<br>24个卷积层 + 2个全连接层<br>交替使用1 × 1卷积 + 3 × 3卷积</p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>![[Yolo V1 loss function.png]]</p>
<h6 id="检测物体的-BB-中心点误差"><a href="#检测物体的-BB-中心点误差" class="headerlink" title="检测物体的 BB 中心点误差"></a>检测物体的 BB 中心点误差</h6><p>λ$_{coord}$ &#x3D;  5</p>
<h6 id="检测物体的-BB-宽高定位误差"><a href="#检测物体的-BB-宽高定位误差" class="headerlink" title="检测物体的 BB 宽高定位误差"></a>检测物体的 BB 宽高定位误差</h6><p>λ$_{coord}$ &#x3D;  5<br>加根号：使小框对误差更敏感</p>
<h6 id="检测物体的-BB-Confidence-误差"><a href="#检测物体的-BB-Confidence-误差" class="headerlink" title="检测物体的 BB Confidence 误差"></a>检测物体的 BB Confidence 误差</h6><p>标签值：BB 与 Ground Truth 的 IoU</p>
<h6 id="不检测物体的-BB-Confidence-误差"><a href="#不检测物体的-BB-Confidence-误差" class="headerlink" title="不检测物体的 BB Confidence 误差"></a>不检测物体的 BB Confidence 误差</h6><p>λ$_{noobj}$ &#x3D;  0.5<br>标签值：0</p>
<h6 id="检测物体的-BB-分类误差"><a href="#检测物体的-BB-分类误差" class="headerlink" title="检测物体的 BB 分类误差"></a>检测物体的 BB 分类误差</h6><h3 id="横向比较"><a href="#横向比较" class="headerlink" title="横向比较"></a>横向比较</h3><h4 id="速度与准确度"><a href="#速度与准确度" class="headerlink" title="速度与准确度"></a>速度与准确度</h4><p>实时目标检测方法中<br>Fast YOLO 最快，YOLO 准确度最高<br>非实时检测方法中<br>YOLO VGG-16 最快</p>
<h4 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h4><p>对小物体和密集物体检测能力较差</p>
<h5 id="和-Fast-RCNN-比较"><a href="#和-Fast-RCNN-比较" class="headerlink" title="和 Fast RCNN 比较"></a>和 Fast RCNN 比较</h5><p>准确率较低，对 Background 和物体的区分能力强<br>精准定位能力差，Localization 误差大</p>
<h5 id="和-Fast-R-CNN-集成"><a href="#和-Fast-R-CNN-集成" class="headerlink" title="和 Fast R-CNN 集成"></a>和 Fast R-CNN 集成</h5><p>Fast R-CNN 和 YOLO 集成有较大提升</p>
<h4 id="迁移泛化能力"><a href="#迁移泛化能力" class="headerlink" title="迁移泛化能力"></a>迁移泛化能力</h4><p>强于同一时期其他网络，尤其是两阶段模型</p>
<h2 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><h4 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h4><h5 id="模型原理"><a href="#模型原理" class="headerlink" title="模型原理"></a>模型原理</h5><p>We present YOLO, a new approach to object detection.<br>我们提出了YOLO，一种新的目标检测方法。<br>Prior work on object detection repurposes classifiers to perform detection.<br>先前关于目标检测的工作重新调整了分类器的用途以执行检测。<br>Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities.<br>相反，我们将目标检测构建为一个回归问题，在划分边界框的同时给出相关的类概率。<br>A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation.<br>单个神经网络在一次评估中直接从完整图像预测边界框和类概率。<br>Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.<br>由于整个检测过程仅使用了单个网络，因此可以直接根据检测性能对其进行端到端优化。</p>
<h5 id="模型优势"><a href="#模型优势" class="headerlink" title="模型优势"></a>模型优势</h5><p>Our unified architecture is extremely fast.<br>我们的一体化架构速度极快。<br>Our base YOLO model processes images in real-time at 45 frames per second.<br>我们的基本YOLO模型以每秒45帧的速度实时处理图像。<br>A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors.<br>该网络的一个较小版本Fast YOLO每秒能够处理惊人的155帧，同时与其他实时检测器相比，仍实现了的两倍mAP。<br>Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background.<br>与最先进的检测系统相比（此处应该是指R-CNN系列模型），YOLO会产生更多的定位误差，但不太可能对背景产生预测误报。<br>Finally, YOLO learns very general representations of objects.<br>最后，YOLO对于学习到的对象具有较强的泛化能力。<br>It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.<br>当从自然图像推广到艺术品等其他领域时，它优于其他检测方法，包括DPM和R-CNN。</p>
<h4 id="摘要重点"><a href="#摘要重点" class="headerlink" title="摘要重点"></a>摘要重点</h4><p>通过单个模型实现端到端目标检测。<br>该模型有极高的目标检测处理速度，在同等速度的模型中具有极高的准确率，而在相近准确度的模型中表现出了极高的处理速度。<br>相较于其他目标检测模型，具有较好的泛化能力。</p>
<h3 id="论文十问"><a href="#论文十问" class="headerlink" title="论文十问"></a>论文十问</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>构建一个统一、实时的端到端目标检测框架。<br>在提升检测速度的同时不过多牺牲其准确率。</p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>在当时其他常用目标检测网络虽然具有较好的准确率，但检测速度较慢。</p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>通过以下模型的构建，能够在有高检测速度的同时不失其准确率。</p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><p>[[AlexNet]]</p>
<h5 id="DPM"><a href="#DPM" class="headerlink" title="DPM"></a>DPM</h5><p>J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable part model for object detection. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2497–2504. IEEE, 2014.</p>
<h5 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h5><p>[[R-CNN]]<br>R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich featrue hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580–587. IEEE, 2014.</p>
<h5 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h5><p>R. B. Girshick. Fast R-CNN. CoRR, abs&#x2F;1504.08083, 2015.</p>
<h5 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h5><p>S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015. 5</p>
<h5 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h5><p>Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng- Yang Fu, and Alexander C. Berg. SSD: Single Shot MultiBox Detector. arXiv preprint arXiv:1512.02325, 2015. </p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>将目标检测问题当作回归问题解决，得以构建端到端预测模型。<br>通过 Grid Cell 分别预测 Bounding Box 并基于 Ground Truth 值进行回归进行模型的训练。<br>通过后处理过程将模型的输出加以分析并将结果反映到图片上。</p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>见上述[[Yolo-V1#整体结构|整体结构]]部分。</p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>数据集为 PASCAL VOC 2007 和 PASCAL VOC 2012。<br>代码已开源：<a target="_blank" rel="noopener" href="https://pjreddie.com/darknet/yolo/">YOLO: Real-Time Object Detection</a></p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>成功通过单个模型实现了端到端目标检测。<br>该模型在同等速度的模型中具有极高的准确率。（提升约100%<del>200%）<br>该模型在相近准确度的模型中表现出了极高的处理速度。（提升约3</del>40倍）</p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>给出了一个全新的目标检测问题的解决方案，该方案相较同类模型具有极高的检测速度。<br>该模型对于有别于训练集的输入与同类模型相比具有更好的泛化能力。</p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>提升模型预测准确度，尤其是对于小目标和密集目标的准确度。<br>进一步优化模型使其预测速度更快。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/VGG%20Net/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/VGG%20Net/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-22 17:11:37" itemprop="dateModified" datetime="2023-03-22T17:11:37+08:00">2023-03-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h3><h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>论文试图解决如何使用深度卷积神经网络（CNN）进行大规模图像识别问题。这并非一个全新的问题，但在此之前，对于非常深的网络结构，训练和优化仍然具有挑战性。  </p>
<h4 id="科学假设"><a href="#科学假设" class="headerlink" title="科学假设"></a>科学假设</h4><p>论文要验证的科学假设是通过使用更深的卷积神经网络（从几层到16-19层），能够在图像识别任务上取得更好的性能。  </p>
<h4 id="相关研究"><a href="#相关研究" class="headerlink" title="相关研究"></a>相关研究</h4><p>[[AlexNet]]<br>相关研究包括深度学习、卷积神经网络（CNN）和图像识别领域的研究。如AlexNet、LeNet等。  </p>
<h4 id="解决方案关键"><a href="#解决方案关键" class="headerlink" title="解决方案关键"></a>解决方案关键</h4><p>论文中的关键解决方案包括：<br>1. 使用更深的卷积神经网络，具有16-19层。<br>2. 使用较小的卷积核（3x3）。<br>3. 在训练过程中使用多尺度训练。  </p>
<h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>论文采用了非常深的卷积神经网络结构，具有16-19层。网络中包含了多个连续的卷积层，后接一个池化层，最后连接一个全连接层进行分类。<br>论文中使用的神经网络结构（VGGNet）包括：<br>1. 多个3x3的卷积层堆叠，实现局部感受野的逐渐扩大；<br>2. 使用更小的卷积核减少参数数量；<br>3. 使用ReLU激活函数；<br>4. 使用2x2的最大池化层进行空间下采样；<br>5. 使用全连接层进行分类。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>论文中使用了交叉熵损失函数：<br>$$<br>L &#x3D; -\sum_{i}y_{i}\log(\hat{y}<em>{i})<br>$$<br>其中，$y_i$ 是真实标签，$\hat{y}</em>{i}$ 是预测标签。  </p>
<h4 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h4><p>论文中的实验设计包括：<br>1. 在ImageNet数据集上训练和评估模型。<br>2. 对不同深度的网络进行对比实验，以确定最佳网络深度。<br>3. 评估所提出的VGGNet与其他现有方法的性能比较。  </p>
<h4 id="数据集和代码"><a href="#数据集和代码" class="headerlink" title="数据集和代码"></a>数据集和代码</h4><p>实验使用了ImageNet数据集进行训练和评估。代码已开源。  </p>
<h4 id="实验结果和假设验证"><a href="#实验结果和假设验证" class="headerlink" title="实验结果和假设验证"></a>实验结果和假设验证</h4><p>实验结果表明，所提出的非常深的卷积神经网络（VGGNet）在图像识别任务上具有很好的性能，支持了论文的假设。  </p>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>论文的具体贡献包括：<br>1. 提出了一个非常深的卷积神经网络结构（VGGNet）。<br>2. 验证了更深的网络结构在图像识别任务上具有更好的性能。<br>3. 提供了一个可以在多个任务中使用的通用特征提取器。</p>
<h4 id="方法缺点"><a href="#方法缺点" class="headerlink" title="方法缺点"></a>方法缺点</h4><p>本文提出的方法存在以下缺点：<br>1. 计算复杂度较高，训练时间较长；<br>2. 参数数量较多，容易导致过拟合；<br>3. 没有考虑模型的计算资源和存储限制。</p>
<h4 id="深入工作"><a href="#深入工作" class="headerlink" title="深入工作"></a>深入工作</h4><p>下一步可以继续深入的工作包括：<br>1. 探索更多的网络结构和优化方法，以进一步提高模型性能。<br>2. 应用所提出的VGGNet结构到其他领域的任务，如目标检测、语义分割等。<br>3. 研究如何减小模型的计算复杂性和内存需求，以便在资源受限的设备上进行部署。<br>4. 尝试与其他模型结合，例如模型融合或迁移学习，以提高在特定任务上的性能。<br>5. 深入了解深度网络的可解释性，以便更好地理解模型学习的特征和决策过程。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/The%20Shattered%20Gradients%20Problem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/The%20Shattered%20Gradients%20Problem/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-20 00:57:09" itemprop="dateModified" datetime="2023-03-20T00:57:09+08:00">2023-03-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>论文试图解决的问题是深度神经网络在训练过程中遇到的梯度消失和梯度爆炸问题，尤其是解释为什么残差网络（ResNets）能够在很深的网络结构中缓解这些问题。  </p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>梯度消失和梯度爆炸问题在深度学习领域并不是一个新问题，但在ResNets出现之前，如何有效地解决这些问题尚不完全明确。  </p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>论文提出了一个假设，即残差网络能够缓解梯度消失和梯度爆炸问题，是因为它们通过跳过连接和恒等映射来减小了网络中不同层之间的梯度间的相互依赖性。这使得梯度在反向传播过程中能够更容易地传播。  </p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><p>相关研究主要包括以下几类：<br>1. 深度神经网络的训练：研究如何有效地训练深度神经网络，例如使用Batch Normalization、Dropout等方法。<br>2. 残差学习：研究残差网络（如ResNet、Pre-Activation ResNet等）及其设计原则，以解决深度神经网络的退化问题。<br>3. 梯度优化：研究不同类型的优化算法和策略，以解决梯度消失和梯度爆炸问题。  </p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>论文的关键在于揭示了残差网络通过跳过连接和恒等映射来减小梯度间的相互依赖性，从而缓解梯度消失和梯度爆炸问题。  </p>
<h4 id="论文中给出的损失函数"><a href="#论文中给出的损失函数" class="headerlink" title="论文中给出的损失函数"></a>论文中给出的损失函数</h4><p>论文中使用了交叉熵损失（Cross-Entropy Loss）作为损失函数，这是许多分类任务中常用的损失函数。  </p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>论文中的实验设计包括：<br>1. 对不同深度的神经网络进行实验，以观察不同深度的模型性能。<br>2. 分析梯度的传播，以揭示残差网络中梯度消失和梯度爆炸问题的缓解机制。  </p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>论文使用了CIFAR-10和CIFAR-100这些广泛使用的图像分类数据集进行定量评估。作者在论文中未提及代码是否开源，但是许多类似的实验可以在开源社区找到实现。  </p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>实验结果支持了论文的假设。作者通过分析梯度的传播，展示了残差网络中跳过连接和恒等映射如何减小梯度间的相互依赖性，从而缓解梯度消失和梯度爆炸问题。  </p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>这篇论文的具体贡献包括：<br>1. 提出了残差网络中跳过连接和恒等映射能够减小梯度间的相互依赖性，从而缓解梯度消失和梯度爆炸问题的观点。<br>2. 分析了不同深度的神经网络中梯度的传播，以揭示残差网络的优势。<br>3. 提供了一个新的视角来理解残差网络为什么能够在很深的网络结构中获得良好的性能，为深度神经网络的训练提供了新的思路。  </p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>1. 探索更多的技巧和方法，以进一步提高残差网络的训练效果和性能。<br>2. 研究不同类型的神经网络结构，以理解类似于残差网络的梯度优化机制是否普遍存在。<br>3. 将这种观察应用于其他领域，例如自然语言处理和强化学习，以了解残差学习在这些领域的有效性。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/ResNet/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-06 15:55:15" itemprop="dateModified" datetime="2023-04-06T15:55:15+08:00">2023-04-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>[[Deep Residual Learning for Image Recognition.pdf]]</p>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><h4 id="传统深度神经网络网络退化"><a href="#传统深度神经网络网络退化" class="headerlink" title="传统深度神经网络网络退化"></a>传统深度神经网络网络退化</h4><p>随着网络深度的增加，训练误差和测试误差可能会反而增加，导致网络性能下降。</p>
<h5 id="梯度消失-x2F-爆炸问题"><a href="#梯度消失-x2F-爆炸问题" class="headerlink" title="梯度消失&#x2F;爆炸问题"></a>梯度消失&#x2F;爆炸问题</h5><p>当网络变得更深时，梯度在反向传播过程中可能会变得非常小（梯度消失）或非常大（梯度爆炸），从而使网络难以训练。这种情况在深度网络中尤为明显，因为梯度在多层之间的传播中可能会被放大或抑制。  </p>
<h5 id="难以优化"><a href="#难以优化" class="headerlink" title="难以优化"></a>难以优化</h5><p>损失函数可能会在高维参数空间中存在许多局部最小值；<br>优化算法难以找到全局最优解。</p>
<h5 id="难以拟合恒等映射"><a href="#难以拟合恒等映射" class="headerlink" title="难以拟合恒等映射"></a>难以拟合恒等映射</h5><p>skip connection 让模型自行选择是否更新；<br>弥补高度非线性造成的信息损失。</p>
<h4 id="ResNet的解决方案"><a href="#ResNet的解决方案" class="headerlink" title="ResNet的解决方案"></a>ResNet的解决方案</h4><p>ResNet（Residual Network）通过引入残差结构来解决网络退化问题。<br>在ResNet中，每个残差模块都由一系列卷积层组成，并添加了一个恒等跳跃连接。（identity skip connection）<br>将输入直接连接到输出，使网络可以学习输入与输出之间的残差函数，而非直接学习输出。   </p>
<h5 id="缓解梯度消失-x2F-爆炸问题"><a href="#缓解梯度消失-x2F-爆炸问题" class="headerlink" title="缓解梯度消失&#x2F;爆炸问题"></a>缓解梯度消失&#x2F;爆炸问题</h5><p>恒等跳跃连接使梯度可以直接在网络层之间传播，从而减轻梯度消失&#x2F;爆炸问题的影响。因此，即使网络非常深，梯度也能更容易地在层之间传播。  </p>
<h5 id="更容易优化"><a href="#更容易优化" class="headerlink" title="更容易优化"></a>更容易优化</h5><p>残差结构使得网络可以学习更简单的残差函数，而非直接学习输出。<br>使网络更容易优化，因为学习残差函数通常比学习输出本身更容易。<br>通过残差结构，网络可以在不引入退化问题的情况下增加深度，从而提高模型性能。</p>
<h5 id="相关文献"><a href="#相关文献" class="headerlink" title="相关文献"></a>相关文献</h5><p>[[Residual Networks Behave Like Ensembles of Relatively Shallow Networks]]<br>[[The Shattered Gradients Problem]]</p>
<h5 id="ResNet结构"><a href="#ResNet结构" class="headerlink" title="ResNet结构"></a>ResNet结构</h5><p>![[ResNet Structure.png]]</p>
<h2 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><h4 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h4><p>Deeper neural networks are more difficult to train.<br>更深层次的神经网络更难训练。<br>We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.<br>我们提出了一个残差学习框架，以简化网络的训练，这些网络比以前使用的网络要深得多。<br>We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, in stead of learning unreferenced functions.<br>我们明确地将层重新表述为参考层输入的学习残差函数，而不是学习未引用的函数。<br>We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.<br>我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从显著增加的深度中获得准确性。<br>On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers — 8× deeper than VGG nets but still having lower complexity.<br>在ImageNet数据集上，我们评估了深度高达152层的残差网络——比VGG网络深8倍，但仍然具有较低的复杂性。<br>An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.<br>这些残差网络的集合在ImageNet测试集上实现了3.57%的误差。这一结果在ILSVRC 2015分类任务中获得了第一名。<br>We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks.<br>我们还对具有100层和1000层的CIFAR-10进行了分析。表示的深度对于许多视觉识别任务来说是至关重要的。<br>Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.<br>完全由于我们非常深入的表示，我们在COCO对象检测数据集上获得了28%的相对改进。<br>Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<br>深度残差网络是我们参加ILSVRC和COCO 2015比赛的基础，在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务中，我们也赢得了第一名。</p>
<h4 id="摘要重点"><a href="#摘要重点" class="headerlink" title="摘要重点"></a>摘要重点</h4><p>提出了一种残差学习框架，以简化更深的神经网络的训练。<br>将网络层重新构造为相对于层输入学习残差函数。<br>残差网络更容易优化，可以从显著增加的深度中获得准确性。  </p>
<h3 id="论文十问"><a href="#论文十问" class="headerlink" title="论文十问"></a>论文十问</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>这篇论文试图解决如何训练更深的神经网络以提高图像识别任务的性能，同时解决更深网络中梯度消失和退化问题。</p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>这并不是一个全新的问题，因为在深度学习领域，研究人员一直在探索如何提高神经网络的性能。然而，该论文提出了一种新的方法，使得在训练更深网络时解决梯度消失和退化问题成为可能。</p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>通过使用残差学习框架，可以训练出更深的神经网络，并在图像识别任务上实现更高的准确性，同时解决梯度消失和退化问题。</p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><p>[[R-CNN]]<br>浅层网络与深度学习的早期方法：如多层感知器(MLP)、卷积神经网络(CNN)等。<br>深度学习的优化方法：如dropout、批量归一化(batch normalization)等。<br>[[VGG Net]]<br>[[AlexNet]]<br>深度神经网络的结构设计：如VGGNet、Inception、DenseNet等。</p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>论文中的关键解决方案是提出了一种名为ResNet（深度残差网络）的网络结构。ResNet通过引入残差连接（skip connections）使得网络层可以学习残差函数，这有助于解决梯度消失和退化问题，从而让网络能够训练得更深。</p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>在ImageNet数据集上进行图像分类任务的实验，对比了不同深度的ResNet与其他先进方法。<br>进一步实验，使用超过1000层的网络，探讨更深网络的训练。<br>将ResNet应用于其他计算机视觉任务，如物体检测（使用COCO数据集）和语义分割（使用PASCAL VOC数据集），以展示其泛化能力。</p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>论文使用的数据集包括ImageNet、COCO和PASCAL VOC。<br>作者在GitHub上开源了ResNet的代码。</p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>ResNet在图像分类任务上实现了高准确性，超过了当时其他先进方法。通过引入残差连接，作者成功地训练了超过1000层的深度网络。此外，将ResNet应用于物体检测和语义分割任务时，性能也得到了显著提升，证明了其在计算机视觉领域的广泛适用性。</p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>提出了一种名为ResNet的深度残差网络，通过引入残差连接，解决了梯度消失和退化问题。<br>成功训练了超过1000层的深度网络，实现了更高的准确性和性能。<br>在多个数据集上进行了实验，展示了ResNet的优越性和泛化能力。<br>开源了ResNet的实现代码，为后续研究和应用提供了基础。</p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>网络结构优化：尝试改进或简化ResNet结构，以进一步提高性能或降低计算复杂度。<br>多任务学习与迁移学习：探究如何利用ResNet进行多任务学习或迁移学习，以提高不同任务间的泛化能力。<br>结合其他优化技术：将ResNet与其他先进的优化方法相结合，如注意力机制、自监督学习等，进一步提高网络性能。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Residual%20Networks%20Behave%20Like%20Ensembles%20of%20Relatively%20Shallow%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Residual%20Networks%20Behave%20Like%20Ensembles%20of%20Relatively%20Shallow%20Networks/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-20 00:54:57" itemprop="dateModified" datetime="2023-03-20T00:54:57+08:00">2023-03-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>论文试图解决的问题是解释残差网络（ResNets）为什么在极深层时仍能够有效地训练并且具有较好的性能。  </p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>这是一个相对较新的问题，因为在ResNets出现之前，如何有效训练极深的神经网络并不明确。  </p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>论文提出并验证了一个假设：ResNets表现得像是相对较浅的网络的集成。也就是说，尽管ResNets具有很深的层次结构，但它们的行为更像是一组较浅层网络的集成，从而解释了为什么它们在训练和性能上如此有效。  </p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><p>相关研究主要包括以下几类：<br>1. 深度神经网络的训练：研究如何有效地训练深度神经网络，例如使用Batch Normalization，Dropout等方法。<br>2. 残差学习：研究残差网络（如ResNet、Pre-Activation ResNet等）及其设计原则，以解决深度神经网络的退化问题。<br>3. 网络集成：研究使用多个神经网络组合的方法来提高模型性能，例如Bagging、Boosting和Stacking等。  </p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>论文的关键在于将ResNets的有效性与它们表现为浅层网络集成之间建立联系。通过在每个残差块之间添加随机dropout，论文将ResNets转换为表现为浅层网络集成的形式。  </p>
<h4 id="论文中给出的损失函数"><a href="#论文中给出的损失函数" class="headerlink" title="论文中给出的损失函数"></a>论文中给出的损失函数</h4><p>论文中使用了交叉熵损失（Cross-Entropy Loss）作为损失函数，这是许多分类任务中常用的损失函数。  </p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>论文中的实验设计包括：<br>1. 对不同深度的ResNets进行实验，以观察不同深度的模型性能。<br>2. 将ResNets与其他基线模型进行比较，例如VGG和GoogLeNet。<br>3. 在每个残差块之间添加随机dropout，以验证ResNets表现为浅层网络集成的假设。  </p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>论文使用了ImageNet、CIFAR-10和CIFAR-100这些广泛使用的数据集进行定量评估。论文的代码并未明确提及是否开源。  </p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>实验结果支持了论文的假设。通过在每个残差块之间添加随机dropout，实验表明ResNets的性能与浅层网络集成的性能相当。实验证实了ResNets在训练和性能上的有效性可以归因于它们表现为浅层网络集成的特性。  </p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>这篇论文的具体贡献包括：<br>1. 提出并验证了ResNets表现得像是相对较浅的网络的集成的假设。<br>2. 通过在残差块之间添加随机dropout来证明假设的正确性。<br>3. 提供了一种新的视角来理解ResNets的有效性，为深度神经网络的训练提供了新的思路。  </p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>1. 探索更多的技巧和方法，以进一步提高ResNets的训练效果和性能。<br>2. 研究不同类型的神经网络结构，以理解类似于ResNets的集成行为是否普遍存在。<br>3. 将这种观察应用于其他领域，例如自然语言处理和强化学习，以了解残差学习在这些领域的有效性。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/R-CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/R-CNN/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-06 15:55:42" itemprop="dateModified" datetime="2023-04-06T15:55:42+08:00">2023-04-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>[[R-CNN.pdf]]</p>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><h3 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h3><h4 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h4><p>约2000个 RoI：Region of Interest<br>基于颜色、纹理、大小和形状的相似性合并相邻的超像素<br>生成候选区域<br>dilate proposal：</p>
<ul>
<li>并向外扩大16px</li>
</ul>
<h4 id="Feature-Extraction"><a href="#Feature-Extraction" class="headerlink" title="Feature Extraction"></a>Feature Extraction</h4><p>统一RoI的大小<br>逐个输入预训练卷积神经网络中提取特征</p>
<h4 id="Classifier-Bounding-Box-Regression"><a href="#Classifier-Bounding-Box-Regression" class="headerlink" title="Classifier + Bounding Box Regression"></a>Classifier + Bounding Box Regression</h4><p>用 SVMs 进行分类<br>进行 Bounding Box 的回归（精修预测框）</p>
<h5 id="SVM-而非-SoftMax"><a href="#SVM-而非-SoftMax" class="headerlink" title="SVM 而非 SoftMax"></a>SVM 而非 SoftMax</h5><p>fine tuning 和训练 SVM 分类器时的正样本不同<br>单独训练分类器，将特征提取与分类任务解耦<br>Fine Tuning：</p>
<ul>
<li>与 GT 的 IoU 最大且大于 0.5 的候选框：正样本</li>
<li>其余候选框：负样本<br>SVM分类器：</li>
<li>GT：正样本</li>
<li>与 GT 的 IoU 小于 0.3：负样本</li>
<li>其余候选框：忽略</li>
</ul>
<h4 id="Non-Maximum-Suppression"><a href="#Non-Maximum-Suppression" class="headerlink" title="Non-Maximum Suppression"></a>Non-Maximum Suppression</h4><p>非极大值抑制来删除重叠的预测框<br>删除与当前预测框有较高IoU（交并比）的其他预测框</p>
<h3 id="特征可视化"><a href="#特征可视化" class="headerlink" title="特征可视化"></a>特征可视化</h3><p>对 Feature Map 中的单个神经元进行反推<br>找到使其产生最大激活的输入<br>考察中心 195 × 195 的 Feature Map</p>
<ul>
<li>有全图感受野</li>
<li>全连接层用以融合特征进行分类</li>
</ul>
<h2 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><h4 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h4><p>Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years.<br>物体检测性能，以典型的PASCAL VOC数据集为衡量标准，在过去几年里已经停滞不前。<br>The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context.<br>最好的方法是复杂的集成系统，通常将多种低级图像特征与高级上下文结合起来。<br>In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%.<br>在本文中，我们提出了一种简单且可扩展的检测算法，相对于VOC 2012上之前最好的结果，提高了平均精度（mAP）超过30％——达到了53.3％的mAP。<br>Our approach combines two key insights:<br>我们的方法结合了两个关键的见解：<br>(1) one can apply high-capacity convolutional neural net works (CNNs) to bottom-up region proposals in order to localize and segment objects and<br>可以将高容量卷积神经网络（CNN）应用于自下而上的区域建议，以定位和分割对象；<br>(2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.<br>当标记训练数据稀缺时，辅助任务的有监督预训练，加特定领域的微调，会显著提高性能。<br>Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features.<br>由于我们将区域建议与CNN结合，因此我们称我们的方法为R-CNN：具有CNN特征的区域。<br>We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features.<br>我们展示了一些实验结果，以揭示网络学习到了什么内容，揭示了丰富层次化的图像特征。<br>Source code for the complete system is available at <a target="_blank" rel="noopener" href="http://www.cs.berkeley.edu/~rbg/rcnn">http://www.cs.berkeley.edu/~rbg/rcnn</a></p>
<h4 id="摘要重点"><a href="#摘要重点" class="headerlink" title="摘要重点"></a>摘要重点</h4><p>本文提出了一种基于区域建议和CNN特征的物体检测算法。<br>本文在PASCAL VOC 2012数据集上取得了显著优于之前方法的性能。<br>本文利用辅助任务进行预训练和微调来克服标记数据稀缺问题。<br>本文探索了CNN学习到了什么样层次化图像特征。</p>
<h3 id="论文十问"><a href="#论文十问" class="headerlink" title="论文十问"></a>论文十问</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>论文试图解决的问题是如何使用卷积神经网络（CNN）提高目标检测性能。</p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>这不是一个新的问题，但是之前的方法基于简单的HOG-like特征，在性能上有限制。</p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>论文要验证的科学假设是通过将区域建议与CNN特征相结合，可以显著提高目标检测的准确度和效率。</p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><p>CNN<br>HOG-like<br>ZF Net：可解释性分析<br>[[ZFNet]]<br>[[AlexNet]]<br>（后续）：<br>SPPNet<br>Fast &#x2F; Faster R-CNN</p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>论文使用名为R-CNN（Regions with Convolutional Neural Networks）的深度学习模型。<br>通过结合选择性搜索（selective search）来生成目标候选区域；<br>使用卷积神经网络（CNN）对这些区域进行特征提取；<br>并利用支持向量机（SVM）进行目标检测。</p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>见上述[[#整体结构]]部分</p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>论文使用的数据集包括PASCAL VOC 2007, 2010, 2011, 2012。<br>代码已开源：<a target="_blank" rel="noopener" href="http://www.cs.berkeley.edu/~rbg/rcnn">http://www.cs.berkeley.edu/~rbg/rcnn</a> 和 GitHub</p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>实验结果支持了论文的科学假设。<br>R-CNN模型在目标检测和语义分割任务上均取得了显著的性能提升。<br>在PASCAL VOC 2007, 2010, 2012的目标检测任务中，R-CNN超过了当时的其他先进方法。<br>在PASCAL VOC 2011的语义分割任务中，R-CNN也取得了显著的性能提升。</p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>提出了一种新的深度学习模型R-CNN，用于目标检测和语义分割任务，结合了选择性搜索、卷积神经网络和支持向量机。<br>通过实验证明了使用基于CNN的深度学习模型自动学习丰富的特征层次结构能够在目标检测和语义分割任务上实现高准确性。<br>在多个数据集（PASCAL VOC 2007, 2010, 2011, 2012）上取得了当时最好的性能。</p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>提高计算效率：R-CNN的计算效率较低，可以尝试优化算法以减少计算时间和资源消耗。<br>多尺度和多任务学习：探究如何利用多尺度特征和多任务学习来进一步提高目标检测和语义分割任务的性能。<br>适应其他领域：将R-CNN应用于其他计算机视觉任务，如实例分割、姿态估计等。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Network%20in%20Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Network%20in%20Network/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-22 13:20:29" itemprop="dateModified" datetime="2023-03-22T13:20:29+08:00">2023-03-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><p>多层感知机（MLP）是一种典型的前馈神经网络，包括输入层、若干个隐藏层和输出层。在NiN中，作者使用了一种特殊形式的MLP，即1x1卷积。这种1x1卷积实际上是一个全连接层，具有线性变换和非线性激活函数。通常，MLP由多个这样的1x1卷积层组成，以增加模型的非线性表达能力。  </p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>传统的卷积层通过空间滤波器（卷积核）来提取局部特征。然而，这种局部响应性使得卷积层的表达能力受到限制，特别是在捕获不同抽象层次特征时。为了克服这个局限性，作者引入了MLP来替换传统卷积核。<br>在NiN中，每个卷积核都被一个小型的多层感知机替代。这意味着每个感受野内的像素都经过一个小型的神经网络（MLP），而不是简单地与卷积核进行线性运算。通过这种方式，网络可以在每个感受野内进行非线性变换，从而增强卷积层的表达能力。  </p>
<h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>引入MLP的主要作用是提高卷积层的表达能力。这主要体现在以下几个方面：<br>1. 增加非线性：MLP由多个1x1卷积层组成，每个1x1卷积层都具有线性变换和非线性激活函数。这使得网络可以学习更复杂的非线性映射，从而捕获更丰富的特征表示。<br>2. 增强局部响应性：通过在每个感受野内进行非线性变换，MLP可以更好地捕获局部特征。这有助于提高网络对不同抽象层次特征的描述能力。<br>3. 降低参数数量：相比于传统的卷积核，使用1x1卷积的MLP可以减少参数数量，从而降低计算复杂度和过拟合的风险。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>多层感知机（MLP）能增强卷积层的表达能力，提高网络对不同抽象层次特征的描述能力。<br>通过这种方法，可以在不显著增加计算复杂度的前提下，进一步提高卷积神经网络的性能。<br>同时，这种方法为后续研究提供了灵活性，可以将MLP与其他卷积神经网络设计技巧相结合，例如残差连接、注意力机制等，以构建更高效、更强大的模型。</p>
<h3 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>“Network in Network”（NiN）论文试图解决深度卷积神经网络（CNN）中局部响应性的问题。局部响应性导致卷积神经网络对于不同抽象层次的特征描述能力受到限制。为了解决这个问题，作者提出了一种新的网络结构，即”Network in Network”，用于增强卷积层的表达能力。  </p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>这是一个相对较新的问题，因为在当时的深度学习领域，大多数研究都集中在设计更深的网络结构以提高性能。而这篇论文关注的是在不增加网络深度的情况下，如何改进卷积层的表达能力。  </p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>论文的主要科学假设是通过在卷积层内部引入微小的多层感知机（MLP），可以提高网络对不同抽象层次特征的描述能力，从而提高整体性能。  </p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><p>[[AlexNet]]<br>[[ResNet]]<br>[[VGG Net]]<br>相关研究主要集中在深度卷积神经网络的设计，例如AlexNet、VGG、ResNet等。这些工作试图通过增加网络深度、引入残差连接等方法来提高网络的性能。  </p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>论文的关键在于在卷积层内部引入多层感知机（MLP），形成了一个新的网络结构：1x1卷积。这种结构允许在每个感受野内进行非线性变换，增强了卷积层的表达能力。  </p>
<h4 id="论文采用的神经网络结构"><a href="#论文采用的神经网络结构" class="headerlink" title="论文采用的神经网络结构"></a>论文采用的神经网络结构</h4><p>论文提出了NiN（Network in Network）结构，其核心是将传统的卷积层替换为含有多层感知机的卷积层。在这种结构中，每个卷积核都是一个小型的多层感知机，这使得网络可以捕捉不同抽象层次的特征。  </p>
<h4 id="论文中给出的损失函数"><a href="#论文中给出的损失函数" class="headerlink" title="论文中给出的损失函数"></a>论文中给出的损失函数</h4><p>论文使用了多任务损失（multi-task loss），包括分类损失和回归损失。分类损失采用交叉熵损失（cross-entropy loss），而回归损失采用均方误差（mean squared error）。  </p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>1. 对比实验：作者将NiN与其他主流卷积神经网络结构进行了对比，如AlexNet、VGG等。<br>可视化实验：作者对NiN模型中的特征图进行了可视化，以展示模型在不同抽象层次上的特征表示。<br>3. 模型融合：作者研究了模型融合对性能的影响，通过将多个NiN模型的预测结果整合，观察对分类准确率的提升。  </p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>论文使用了CIFAR-10和CIFAR-100数据集进行定量评估。此外，作者还在ILSVRC2012数据集上进行了实验。代码已开源，并可在GitHub上找到。  </p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>实验结果表明，NiN模型在CIFAR-10、CIFAR-100和ILSVRC2012数据集上的性能优于其他主流卷积神经网络结构，如AlexNet和VGG。这些结果支持了论文的主要假设，即通过在卷积层内部引入多层感知机可以提高网络的表达能力。  </p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>1. 提出了一种新颖的卷积神经网络结构：Network in Network（NiN），通过在卷积层内部引入多层感知机增强了表达能力。<br>2. 实验证明了NiN在图像分类任务上具有优越的性能。<br>3. 可视化实验展示了NiN模型在不同抽象层次上的特征表示。  </p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>1. 将NiN结构应用于其他计算机视觉任务，例如目标检测、语义分割等。<br>2. 结合其他卷积神经网络设计技巧，如残差连接、注意力机制等，进一步提高NiN的性能。<br>3. 探索更加高效的多层感知机结构和激活函数，以改进NiN的表达能力。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Inception-V4%20and%20InceptionResNet-V2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Inception-V4%20and%20InceptionResNet-V2/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-06 15:55:30" itemprop="dateModified" datetime="2023-04-06T15:55:30+08:00">2023-04-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>[[Inception-V4 and InceptionResNet-V2.pdf]]</p>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><h3 id="Inception单元"><a href="#Inception单元" class="headerlink" title="Inception单元"></a>Inception单元</h3><h4 id="基础思路"><a href="#基础思路" class="headerlink" title="基础思路"></a>基础思路</h4><p>多尺度并行处理<br>concatenation合并传入下一层<br>改进：1×1卷积核降维</p>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>![[Inception卷积层.png]]<br>减少concat后的层数，降低计算量</p>
<h4 id="卷积层改进"><a href="#卷积层改进" class="headerlink" title="卷积层改进"></a>卷积层改进</h4><p>5×5卷积 -&gt; 两个3×3卷积<br>7×7卷积 -&gt; 三个3×3卷积</p>
<h5 id="Inception-Module-A"><a href="#Inception-Module-A" class="headerlink" title="Inception Module A"></a>Inception Module A</h5><p>![[Inception Module A.png]]</p>
<h4 id="不对称卷积"><a href="#不对称卷积" class="headerlink" title="不对称卷积"></a>不对称卷积</h4><p>n×n卷积 -&gt; 一个1×n卷积 + 1个n×1卷积</p>
<h5 id="Inception-Module-B"><a href="#Inception-Module-B" class="headerlink" title="Inception Module B"></a>Inception Module B</h5><p>深度方向分解<br>![[Inception Module B.png]]</p>
<h5 id="Inception-Module-C"><a href="#Inception-Module-C" class="headerlink" title="Inception Module C"></a>Inception Module C</h5><p>宽度方向分解<br>增加表示的维度，增加channel数<br>进入分类层前扩增通道数<br>![[Inception Module C.png]]</p>
<h4 id="降采样模块"><a href="#降采样模块" class="headerlink" title="降采样模块"></a>降采样模块</h4><h5 id="Reduction-A"><a href="#Reduction-A" class="headerlink" title="Reduction A"></a>Reduction A</h5><p>35×35降采样到17×17<br>![[Reduction A.png]]</p>
<h5 id="Reduction-B"><a href="#Reduction-B" class="headerlink" title="Reduction B"></a>Reduction B</h5><p>17×17降采样到8×8<br>![[Reduction B.png]]</p>
<h3 id="InceptionV4结构"><a href="#InceptionV4结构" class="headerlink" title="InceptionV4结构"></a>InceptionV4结构</h3><h4 id="主体结构"><a href="#主体结构" class="headerlink" title="主体结构"></a>主体结构</h4><h5 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h5><p>![[InceptionV4.png]]</p>
<h5 id="辅助分类头"><a href="#辅助分类头" class="headerlink" title="辅助分类头"></a>辅助分类头</h5><p>Inception B之后</p>
<h5 id="主分类头"><a href="#主分类头" class="headerlink" title="主分类头"></a>主分类头</h5><p>全局平均池化<br>8×8 -&gt; 1×1<br>然后Dropout + 全连接层</p>
<h5 id="Stem部分"><a href="#Stem部分" class="headerlink" title="Stem部分"></a>Stem部分</h5><p>![[InceptionV4 Stem.png]]</p>
<h3 id="InceptionResNet-V2改进"><a href="#InceptionResNet-V2改进" class="headerlink" title="InceptionResNet-V2改进"></a>InceptionResNet-V2改进</h3><h4 id="Residual缩放"><a href="#Residual缩放" class="headerlink" title="Residual缩放"></a>Residual缩放</h4><p>残差层在滤波器的数量超过1000时，在多次迭代后，在平均池化前仅产生0值。<br>该问题无法通过降低学习率或是向该层添加额外的批标准化来防止。<br>通过在添加到激活层前对残差层的值进行一定的缩放（×0.1~×0.3）解决。</p>
<h3 id="最终性能"><a href="#最终性能" class="headerlink" title="最终性能"></a>最终性能</h3><h4 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h4><h5 id="多帧评估"><a href="#多帧评估" class="headerlink" title="多帧评估"></a>多帧评估</h5><p>通过缩放旋转等方式进行变换后，对同一图片进行预测<br>论文中：144帧</p>
<h5 id="多模型集成"><a href="#多模型集成" class="headerlink" title="多模型集成"></a>多模型集成</h5><p>3个InceptionResV2 + 1个InceptionV4集成<br>进一步提升准确率</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>计算量略大</p>
<h2 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><h4 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h4><h5 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h5><p>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years.<br>深度卷积网络是近年来图像识别性能最大进步的核心。<br>One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost.<br>一个例子是Inception架构，该架构已被证明以相对较低的计算成本实现了非常好的性能。<br>Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network.<br>最近，残差连接的引入与传统的架构相结合，在 2015 年 ILSVRC 挑战赛中产生了最先进的性能；它的性能类似于最新一代的Inception-v3网络。<br>This raises the question of whether there are any benefit in combining the Inception architecture with residual connections.<br>这就提出了一个问题，即将Inception架构与残差连接相结合是否有好处。</p>
<h5 id="提出改进"><a href="#提出改进" class="headerlink" title="提出改进"></a>提出改进</h5><p>Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly.<br>在这里，我们给出了明确的证据，即具有残差连接的训练显著加速了初始网络的训练。<br>There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin.<br>还有一些证据表明，在没有残差连接的情况下，使用残差连接的Inception网络以微弱优势优于同样计算代价的Inception网络。<br>We also present several new streamlined architectures for both residual and non-residual Inception networks.<br>我们还为残差和非残差Inception网络提出了几种新的精简架构。<br>These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly.<br>这些变化显著提高了ILSVRC 2012分类任务的单帧识别性能。<br>We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks.<br>我们进一步证明了适当的激活缩放如何稳定对于非常广泛的残差初始网络的训练。<br>With an ensemble of three residual and one Inception-v4, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.<br>通过三个InceptionResnet-v2和一个Inception-v4的集成，我们在ImageNet分类（CLS）挑战的测试集上获得了3.08%的前5位错误。</p>
<h4 id="摘要重点"><a href="#摘要重点" class="headerlink" title="摘要重点"></a>摘要重点</h4><p>既有结构Inception和ResNet残差架构结合，大幅提升训练速度并小幅提升精确度<br>精简了神经网络的架构，大幅提升单帧识别性能<br>对残差求和前，按一定比例缩放，解决网络不稳定的问题<br>InceptionResnet和Inception两种模型集成，提升识别准确度</p>
<h3 id="论文十问"><a href="#论文十问" class="headerlink" title="论文十问"></a>论文十问</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>基于前代Inception网络，在此之上引入了残差网络结构，并进一步优化。</p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>结合最新创新的网络优化。</p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>通过以下优化，能够显著提升深度卷积神经网络的Top-5预测准确度。</p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><h5 id="Network-in-Network"><a href="#Network-in-Network" class="headerlink" title="Network in Network"></a>Network in Network</h5><p>M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.</p>
<h5 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h5><p>[[VGG Net]]<br>K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.</p>
<h5 id="GoogLeNet-Inception-v1"><a href="#GoogLeNet-Inception-v1" class="headerlink" title="GoogLeNet (Inception-v1)"></a>GoogLeNet (Inception-v1)</h5><p>C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015.</p>
<h5 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h5><p>[[ResNet]]<br>K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.</p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>将ResNet的残差网络结构与Inception结构结合以期在保持其计算效率的同时，获得残差网络的所有优点。<br>能否通过扩展Inception结构的宽度和深度来获得更好的效率。<br>在迁移至TensorFlow后，能否对整体架构进行简化。<br>能否通过多模型融合获得更好的效果。</p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>见上述[[Inception-V4 and InceptionResNet-V2#整体结构|整体结构]]部分</p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>数据集为ILSVRC 2012。<br>代码已开源：<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py">Inception-V4</a>, <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py">InceptionResNet-V2</a></p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>通过引入残差网络结构，在相同的模型规模下，提升了训练速度，小幅度提升了准确率。<br>Inception-V4和InceptionResNet-V2超过了当时表现最好的ResNet-151。<br>通过模型融合，进一步将Top-5 Error从3.8%降至3.1%。</p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>提出了当时图像识别领域准确率最高的Inception-V4和InceptionResNet-V2模型。<br>指出残差网络的引入显著提高了Inception架构的训练速度。</p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>可能的更深的网络结构；对残差网络的进一步应用。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Feature%20Pyramid%20Networks%20for%20Object%20Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CarlosDJY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Feature%20Pyramid%20Networks%20for%20Object%20Detection/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-20 12:00:54" itemprop="dateModified" datetime="2023-03-20T12:00:54+08:00">2023-03-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>该论文试图解决在物体检测任务中，如何利用卷积神经网络（CNN）在不同尺度上进行特征提取，以提高检测性能。  </p>
<h4 id="是否是一个新的问题"><a href="#是否是一个新的问题" class="headerlink" title="是否是一个新的问题"></a>是否是一个新的问题</h4><p>这不是一个新的问题，因为在物体检测任务中，处理不同尺度的物体一直是一个重要挑战。过去的研究也已经探讨了多尺度表示的问题。  </p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>论文的主要科学假设是：通过构建一个特征金字塔网络（FPN），可以有效地将高级语义特征与低级特征相结合，以提高物体检测性能。  </p>
<h4 id="相关研究及归类"><a href="#相关研究及归类" class="headerlink" title="相关研究及归类"></a>相关研究及归类</h4><p>[[Faster R-CNN]]<br>相关研究包括金字塔表示法、空间金字塔池化（SPP）和卷积层级联（ConvNets）。这些方法可以归类为多尺度表示方法，试图解决物体检测任务中不同尺度物体的问题。  </p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>论文中的关键解决方案是提出了一种名为特征金字塔网络（FPN）的新型网络结构。FPN 通过自上而下的路径和横向连接，将高层次的丰富语义信息传递到低层次，并保持了高分辨率。这使得 FPN 能够在多尺度上进行有效的物体检测。  </p>
<h4 id="论文采用的神经网络结构"><a href="#论文采用的神经网络结构" class="headerlink" title="论文采用的神经网络结构"></a>论文采用的神经网络结构</h4><p>论文采用的神经网络结构是基于卷积神经网络的特征金字塔网络（FPN）。FPN 使用自上而下的路径和横向连接，将不同层次的特征进行融合。  </p>
<h4 id="论文中给出的损失函数"><a href="#论文中给出的损失函数" class="headerlink" title="论文中给出的损失函数"></a>论文中给出的损失函数</h4><p>论文在基于 FPN 的 Faster R-CNN 模型上采用了多任务损失函数，包括分类损失和回归损失。  </p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>论文中的实验主要有以下几个方面：<br>1. 评估 FPN 在 COCO 数据集上的物体检测性能。<br>2. 比较 FPN 与其他多尺度表示方法的性能。<br>3. 分析 FPN 的各组件对性能的影响。<br>4. 在 PASCAL VOC 数据集上验证 FPN 的泛化能力。  </p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>定量评估的数据集包括 COCO 数据集和 PASCAL VOC 数据集。论文作者已经开源了代码。  </p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>实验结果支持论文的假设。FPN 在物体检测任务上取得了显著的性能提升，表明将高层次的丰富语义信息传递到低层次，并保持高分辨率，确实有助于提高物体检测性能。此外，与其他多尺度表示方法相比，FPN 也表现出了更好的性能。  </p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>这篇论文的具体贡献有以下几点：<br>1. 提出了一种新型的特征金字塔网络（FPN），有效地解决了物体检测任务中不同尺度物体的问题。<br>2. 通过自上而下的路径和横向连接，实现了高层次语义信息与低层次特征的融合。<br>3. 在 COCO 和 PASCAL VOC 数据集上进行了大量实验证明 FPN 的有效性，取得了显著的性能提升。<br>4. 与其他多尺度表示方法进行了对比，证明了 FPN 的优越性。  </p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>下一步可以在以下方面继续深入研究：<br>1. 探讨 FPN 在其他计算机视觉任务，如语义分割、人体姿态估计等方面的应用。<br>2. 研究更加有效的特征融合策略，以进一步提高多尺度表示的性能。<br>3. 优化 FPN 的网络结构，以降低计算复杂度和提高计算效率。<br>4. 结合其他物体检测方法，如 anchor-free、one-stage 等方法，进一步提升检测性能。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CarlosDJY</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
