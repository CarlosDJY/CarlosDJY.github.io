<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"carlosdjy.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.17.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="[[Deep Residual Learning for Image Recognition.pdf]] 整体结构传统深度神经网络网络退化随着网络深度的增加，训练误差和测试误差可能会反而增加，导致网络性能下降。 梯度消失&#x2F;爆炸问题当网络变得更深时，梯度在反向传播过程中可能会变得非常小（梯度消失）或非常大（梯度爆炸），从而使网络难以训练。这种情况在深度网络中尤为明显，因为梯度在多层之间的传">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/ResNet/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="[[Deep Residual Learning for Image Recognition.pdf]] 整体结构传统深度神经网络网络退化随着网络深度的增加，训练误差和测试误差可能会反而增加，导致网络性能下降。 梯度消失&#x2F;爆炸问题当网络变得更深时，梯度在反向传播过程中可能会变得非常小（梯度消失）或非常大（梯度爆炸），从而使网络难以训练。这种情况在深度网络中尤为明显，因为梯度在多层之间的传">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-06-16T13:28:02.563Z">
<meta property="article:modified_time" content="2023-04-06T07:55:15.371Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/ResNet/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/ResNet/","path":"2023/06/16/机器学习/CNN/Image Recognition/ResNet/","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title> | Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">整体结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BD%91%E7%BB%9C%E9%80%80%E5%8C%96"><span class="nav-number">1.0.1.</span> <span class="nav-text">传统深度神经网络网络退化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-x2F-%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98"><span class="nav-number">1.0.1.1.</span> <span class="nav-text">梯度消失&#x2F;爆炸问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%BE%E4%BB%A5%E4%BC%98%E5%8C%96"><span class="nav-number">1.0.1.2.</span> <span class="nav-text">难以优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%BE%E4%BB%A5%E6%8B%9F%E5%90%88%E6%81%92%E7%AD%89%E6%98%A0%E5%B0%84"><span class="nav-number">1.0.1.3.</span> <span class="nav-text">难以拟合恒等映射</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">1.0.2.</span> <span class="nav-text">ResNet的解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%93%E8%A7%A3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-x2F-%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98"><span class="nav-number">1.0.2.1.</span> <span class="nav-text">缓解梯度消失&#x2F;爆炸问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9B%B4%E5%AE%B9%E6%98%93%E4%BC%98%E5%8C%96"><span class="nav-number">1.0.2.2.</span> <span class="nav-text">更容易优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE"><span class="nav-number">1.0.2.3.</span> <span class="nav-text">相关文献</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ResNet%E7%BB%93%E6%9E%84"><span class="nav-number">1.0.2.4.</span> <span class="nav-text">ResNet结构</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A"><span class="nav-number">2.</span> <span class="nav-text">论文报告</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">2.1.</span> <span class="nav-text">摘要</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91"><span class="nav-number">2.1.1.</span> <span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E9%87%8D%E7%82%B9"><span class="nav-number">2.1.2.</span> <span class="nav-text">摘要重点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE"><span class="nav-number">2.2.</span> <span class="nav-text">论文十问</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98"><span class="nav-number">2.2.1.</span> <span class="nav-text">论文试图解决什么问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E6%98%AF%E5%90%A6%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">2.2.2.</span> <span class="nav-text">这是否是一个新的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E8%A6%81%E9%AA%8C%E8%AF%81%E4%BB%80%E4%B9%88%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE"><span class="nav-number">2.2.3.</span> <span class="nav-text">论文要验证什么科学假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E5%93%AA%E4%BA%9B%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E5%A6%82%E4%BD%95%E5%BD%92%E7%B1%BB"><span class="nav-number">2.2.4.</span> <span class="nav-text">有哪些相关研究，如何归类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E4%B8%AD%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%85%B3%E9%94%AE"><span class="nav-number">2.2.5.</span> <span class="nav-text">论文中解决方案的关键</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E4%B8%AD%E5%AE%9E%E9%AA%8C%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="nav-number">2.2.6.</span> <span class="nav-text">论文中实验的设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E9%87%8F%E8%AF%84%E4%BC%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BB%A3%E7%A0%81%E6%98%AF%E5%90%A6%E5%BC%80%E6%BA%90"><span class="nav-number">2.2.7.</span> <span class="nav-text">定量评估的数据集，代码是否开源</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E5%8F%8A%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E6%94%AF%E6%8C%81%E9%9C%80%E9%AA%8C%E8%AF%81%E7%9A%84%E5%81%87%E8%AE%BE"><span class="nav-number">2.2.8.</span> <span class="nav-text">实验及结果是否支持需验证的假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E7%9A%84%E5%85%B7%E4%BD%93%E8%B4%A1%E7%8C%AE"><span class="nav-number">2.2.9.</span> <span class="nav-text">这篇论文的具体贡献</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%8F%AF%E4%BB%A5%E7%BB%A7%E7%BB%AD%E6%B7%B1%E5%85%A5%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.2.10.</span> <span class="nav-text">下一步可以继续深入的工作</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://carlosdjy.github.io/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-16 21:28:02" itemprop="dateCreated datePublished" datetime="2023-06-16T21:28:02+08:00">2023-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-06 15:55:15" itemprop="dateModified" datetime="2023-04-06T15:55:15+08:00">2023-04-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>[[Deep Residual Learning for Image Recognition.pdf]]</p>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><h4 id="传统深度神经网络网络退化"><a href="#传统深度神经网络网络退化" class="headerlink" title="传统深度神经网络网络退化"></a>传统深度神经网络网络退化</h4><p>随着网络深度的增加，训练误差和测试误差可能会反而增加，导致网络性能下降。</p>
<h5 id="梯度消失-x2F-爆炸问题"><a href="#梯度消失-x2F-爆炸问题" class="headerlink" title="梯度消失&#x2F;爆炸问题"></a>梯度消失&#x2F;爆炸问题</h5><p>当网络变得更深时，梯度在反向传播过程中可能会变得非常小（梯度消失）或非常大（梯度爆炸），从而使网络难以训练。这种情况在深度网络中尤为明显，因为梯度在多层之间的传播中可能会被放大或抑制。  </p>
<h5 id="难以优化"><a href="#难以优化" class="headerlink" title="难以优化"></a>难以优化</h5><p>损失函数可能会在高维参数空间中存在许多局部最小值；<br>优化算法难以找到全局最优解。</p>
<h5 id="难以拟合恒等映射"><a href="#难以拟合恒等映射" class="headerlink" title="难以拟合恒等映射"></a>难以拟合恒等映射</h5><p>skip connection 让模型自行选择是否更新；<br>弥补高度非线性造成的信息损失。</p>
<h4 id="ResNet的解决方案"><a href="#ResNet的解决方案" class="headerlink" title="ResNet的解决方案"></a>ResNet的解决方案</h4><p>ResNet（Residual Network）通过引入残差结构来解决网络退化问题。<br>在ResNet中，每个残差模块都由一系列卷积层组成，并添加了一个恒等跳跃连接。（identity skip connection）<br>将输入直接连接到输出，使网络可以学习输入与输出之间的残差函数，而非直接学习输出。   </p>
<h5 id="缓解梯度消失-x2F-爆炸问题"><a href="#缓解梯度消失-x2F-爆炸问题" class="headerlink" title="缓解梯度消失&#x2F;爆炸问题"></a>缓解梯度消失&#x2F;爆炸问题</h5><p>恒等跳跃连接使梯度可以直接在网络层之间传播，从而减轻梯度消失&#x2F;爆炸问题的影响。因此，即使网络非常深，梯度也能更容易地在层之间传播。  </p>
<h5 id="更容易优化"><a href="#更容易优化" class="headerlink" title="更容易优化"></a>更容易优化</h5><p>残差结构使得网络可以学习更简单的残差函数，而非直接学习输出。<br>使网络更容易优化，因为学习残差函数通常比学习输出本身更容易。<br>通过残差结构，网络可以在不引入退化问题的情况下增加深度，从而提高模型性能。</p>
<h5 id="相关文献"><a href="#相关文献" class="headerlink" title="相关文献"></a>相关文献</h5><p>[[Residual Networks Behave Like Ensembles of Relatively Shallow Networks]]<br>[[The Shattered Gradients Problem]]</p>
<h5 id="ResNet结构"><a href="#ResNet结构" class="headerlink" title="ResNet结构"></a>ResNet结构</h5><p>![[ResNet Structure.png]]</p>
<h2 id="论文报告"><a href="#论文报告" class="headerlink" title="论文报告"></a>论文报告</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><h4 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h4><p>Deeper neural networks are more difficult to train.<br>更深层次的神经网络更难训练。<br>We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.<br>我们提出了一个残差学习框架，以简化网络的训练，这些网络比以前使用的网络要深得多。<br>We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, in stead of learning unreferenced functions.<br>我们明确地将层重新表述为参考层输入的学习残差函数，而不是学习未引用的函数。<br>We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.<br>我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从显著增加的深度中获得准确性。<br>On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers — 8× deeper than VGG nets but still having lower complexity.<br>在ImageNet数据集上，我们评估了深度高达152层的残差网络——比VGG网络深8倍，但仍然具有较低的复杂性。<br>An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.<br>这些残差网络的集合在ImageNet测试集上实现了3.57%的误差。这一结果在ILSVRC 2015分类任务中获得了第一名。<br>We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks.<br>我们还对具有100层和1000层的CIFAR-10进行了分析。表示的深度对于许多视觉识别任务来说是至关重要的。<br>Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.<br>完全由于我们非常深入的表示，我们在COCO对象检测数据集上获得了28%的相对改进。<br>Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<br>深度残差网络是我们参加ILSVRC和COCO 2015比赛的基础，在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务中，我们也赢得了第一名。</p>
<h4 id="摘要重点"><a href="#摘要重点" class="headerlink" title="摘要重点"></a>摘要重点</h4><p>提出了一种残差学习框架，以简化更深的神经网络的训练。<br>将网络层重新构造为相对于层输入学习残差函数。<br>残差网络更容易优化，可以从显著增加的深度中获得准确性。  </p>
<h3 id="论文十问"><a href="#论文十问" class="headerlink" title="论文十问"></a>论文十问</h3><h4 id="论文试图解决什么问题"><a href="#论文试图解决什么问题" class="headerlink" title="论文试图解决什么问题"></a>论文试图解决什么问题</h4><p>这篇论文试图解决如何训练更深的神经网络以提高图像识别任务的性能，同时解决更深网络中梯度消失和退化问题。</p>
<h4 id="这是否是一个新的问题"><a href="#这是否是一个新的问题" class="headerlink" title="这是否是一个新的问题"></a>这是否是一个新的问题</h4><p>这并不是一个全新的问题，因为在深度学习领域，研究人员一直在探索如何提高神经网络的性能。然而，该论文提出了一种新的方法，使得在训练更深网络时解决梯度消失和退化问题成为可能。</p>
<h4 id="论文要验证什么科学假设"><a href="#论文要验证什么科学假设" class="headerlink" title="论文要验证什么科学假设"></a>论文要验证什么科学假设</h4><p>通过使用残差学习框架，可以训练出更深的神经网络，并在图像识别任务上实现更高的准确性，同时解决梯度消失和退化问题。</p>
<h4 id="有哪些相关研究，如何归类"><a href="#有哪些相关研究，如何归类" class="headerlink" title="有哪些相关研究，如何归类"></a>有哪些相关研究，如何归类</h4><p>[[R-CNN]]<br>浅层网络与深度学习的早期方法：如多层感知器(MLP)、卷积神经网络(CNN)等。<br>深度学习的优化方法：如dropout、批量归一化(batch normalization)等。<br>[[VGG Net]]<br>[[AlexNet]]<br>深度神经网络的结构设计：如VGGNet、Inception、DenseNet等。</p>
<h4 id="论文中解决方案的关键"><a href="#论文中解决方案的关键" class="headerlink" title="论文中解决方案的关键"></a>论文中解决方案的关键</h4><p>论文中的关键解决方案是提出了一种名为ResNet（深度残差网络）的网络结构。ResNet通过引入残差连接（skip connections）使得网络层可以学习残差函数，这有助于解决梯度消失和退化问题，从而让网络能够训练得更深。</p>
<h4 id="论文中实验的设计"><a href="#论文中实验的设计" class="headerlink" title="论文中实验的设计"></a>论文中实验的设计</h4><p>在ImageNet数据集上进行图像分类任务的实验，对比了不同深度的ResNet与其他先进方法。<br>进一步实验，使用超过1000层的网络，探讨更深网络的训练。<br>将ResNet应用于其他计算机视觉任务，如物体检测（使用COCO数据集）和语义分割（使用PASCAL VOC数据集），以展示其泛化能力。</p>
<h4 id="定量评估的数据集，代码是否开源"><a href="#定量评估的数据集，代码是否开源" class="headerlink" title="定量评估的数据集，代码是否开源"></a>定量评估的数据集，代码是否开源</h4><p>论文使用的数据集包括ImageNet、COCO和PASCAL VOC。<br>作者在GitHub上开源了ResNet的代码。</p>
<h4 id="实验及结果是否支持需验证的假设"><a href="#实验及结果是否支持需验证的假设" class="headerlink" title="实验及结果是否支持需验证的假设"></a>实验及结果是否支持需验证的假设</h4><p>ResNet在图像分类任务上实现了高准确性，超过了当时其他先进方法。通过引入残差连接，作者成功地训练了超过1000层的深度网络。此外，将ResNet应用于物体检测和语义分割任务时，性能也得到了显著提升，证明了其在计算机视觉领域的广泛适用性。</p>
<h4 id="这篇论文的具体贡献"><a href="#这篇论文的具体贡献" class="headerlink" title="这篇论文的具体贡献"></a>这篇论文的具体贡献</h4><p>提出了一种名为ResNet的深度残差网络，通过引入残差连接，解决了梯度消失和退化问题。<br>成功训练了超过1000层的深度网络，实现了更高的准确性和性能。<br>在多个数据集上进行了实验，展示了ResNet的优越性和泛化能力。<br>开源了ResNet的实现代码，为后续研究和应用提供了基础。</p>
<h4 id="下一步可以继续深入的工作"><a href="#下一步可以继续深入的工作" class="headerlink" title="下一步可以继续深入的工作"></a>下一步可以继续深入的工作</h4><p>网络结构优化：尝试改进或简化ResNet结构，以进一步提高性能或降低计算复杂度。<br>多任务学习与迁移学习：探究如何利用ResNet进行多任务学习或迁移学习，以提高不同任务间的泛化能力。<br>结合其他优化技术：将ResNet与其他先进的优化方法相结合，如注意力机制、自监督学习等，进一步提高网络性能。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/Residual%20Networks%20Behave%20Like%20Ensembles%20of%20Relatively%20Shallow%20Networks/" rel="prev" title="">
                  <i class="fa fa-chevron-left"></i> 
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CNN/Image%20Recognition/The%20Shattered%20Gradients%20Problem/" rel="next" title="">
                   <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
